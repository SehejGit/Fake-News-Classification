{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521592f5",
   "metadata": {},
   "source": [
    "# Logistic Regression Model to Classify Real/Fake News Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3da97",
   "metadata": {},
   "source": [
    "## Notebook Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a4508",
   "metadata": {},
   "source": [
    "- Load the Kaggle and API data from GCS to Mongo\n",
    "- Combine them on Mongo\n",
    "- Load random 100 rows from combined_data to notebook and fit a model\n",
    "- Because it is random, I set a threshold of 50% for accuracy\n",
    "- The function runs for 3 times for better score and breaks\n",
    "- It then loads the test file, randomly picks 50 rows and predicts whether it is real/fake and creates a CSV file\n",
    "- This predicted file is loaded back to Mongo & GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268ead0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1c7a178-f1d1-4a95-8cfa-4789cb9f326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, length, monotonically_increasing_id\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pymongo\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import tempfile\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b027730",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a58f289-8f39-42ef-a14b-9095d226a709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 21:20:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# # Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize Spark Session with optimized memory settings\n",
    "spark = SparkSession.builder.appName('test').config(\"spark.driver.memory\", \"10g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5199f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with GCP configurations\n",
    "def initialize_spark_with_gcp():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('fake_news_classifier') \\\n",
    "        .config(\"spark.driver.memory\", \"10g\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,com.google.cloud:google-cloud-storage:2.20.1\") \\\n",
    "        .getOrCreate()\n",
    "   \n",
    "    return spark\n",
    "\n",
    "# MongoDB connection parameters with GCP connection\n",
    "MONGO_URI = \"mongodb+srv://bayanenisamanvithachowdary:<PASSWORD>@samtesting.rmc9m.mongodb.net/\"\n",
    "DB_NAME = \"fake_news_db\"\n",
    "KAGGLE_COLLECTION = \"kaggle_data\"\n",
    "API_COLLECTION = \"api_data\"\n",
    "COMBINED_COLLECTION = \"combined_data\"\n",
    "PREDICTIONS_COLLECTION = \"predictions\"\n",
    "\n",
    "# GCP Storage bucket information\n",
    "GCP_BUCKET_NAME = \"testing694\"  # Replace with your GCP bucket name\n",
    "GCS_KAGGLE_FILE = \"kaggle_datasets_full.csv\"  # Path in your bucket\n",
    "GCS_API_FILE = \"API_datasets_full.csv\"  # Path in your bucket\n",
    "GCS_GNEWS_FILE = \"gnews_dataset.csv\"  # Path in your bucket\n",
    "\n",
    "# In your Python code, explicitly specify the path to your service account key\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/samanvitha/Downloads/crafty-academy-452602-i1-b2f4d1be2ef2.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f507b",
   "metadata": {},
   "source": [
    "## Functions to Execute the Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e17cb",
   "metadata": {},
   "source": [
    "### Combining Data in MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b671169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_from_gcs_to_mongodb(bucket_name, file_path, collection_name):\n",
    "    \"\"\"Load CSV data from Google Cloud Storage into MongoDB\"\"\"\n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(file_path)\n",
    "        \n",
    "        # Download to a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:\n",
    "            blob.download_to_filename(temp_file.name)\n",
    "            temp_filepath = temp_file.name\n",
    "        \n",
    "        # Read CSV with Spark\n",
    "        spark = initialize_spark_with_gcp()\n",
    "        df = spark.read.csv(temp_filepath, header=True, inferSchema=True).limit(1000)\n",
    "        \n",
    "        # Convert to pandas for easier MongoDB insertion\n",
    "        pandas_df = df.toPandas()\n",
    "        \n",
    "        # Connect to MongoDB\n",
    "        client = pymongo.MongoClient(MONGO_URI)\n",
    "        db = client[DB_NAME]\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        # Delete existing data\n",
    "        collection.delete_many({})\n",
    "        \n",
    "        # Insert data\n",
    "        collection.insert_many(pandas_df.to_dict('records'))\n",
    "        \n",
    "        # Clean up temp file\n",
    "        os.unlink(temp_filepath)\n",
    "        \n",
    "        print(f\"Loaded {len(pandas_df)} records from GCS:{bucket_name}/{file_path} to {collection_name}\")\n",
    "        client.close()\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from GCS to MongoDB: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d251902a-0393-45af-a8d0-929e81810694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_aggregate_mongodb_data():\n",
    "    \"\"\"Combine data from both collections and perform aggregation\"\"\"\n",
    "    try:\n",
    "        # Connect to MongoDB\n",
    "        client = pymongo.MongoClient(MONGO_URI)\n",
    "        db = client[DB_NAME]\n",
    "        \n",
    "        # Get data from both collections\n",
    "        kaggle_data = list(db[KAGGLE_COLLECTION].find())\n",
    "        api_data = list(db[API_COLLECTION].find())\n",
    "        \n",
    "        # Ensure we have common fields\n",
    "        combined_data = []\n",
    "        \n",
    "        for item in kaggle_data:\n",
    "            if 'text' in item and 'label' in item:\n",
    "                # Basic cleaning\n",
    "                if isinstance(item['text'], str) and len(item['text']) > 10:\n",
    "                    combined_data.append({\n",
    "                        'source': 'kaggle',\n",
    "                        'text': item['text'],\n",
    "                        'label': item['label'],\n",
    "                        'timestamp': datetime.now()\n",
    "                    })\n",
    "        \n",
    "        for item in api_data:\n",
    "            if 'text' in item and 'label' in item:\n",
    "                # Basic cleaning\n",
    "                if isinstance(item['text'], str) and len(item['text']) > 10:\n",
    "                    combined_data.append({\n",
    "                        'source': 'api',\n",
    "                        'text': item['text'],\n",
    "                        'label': item['label'],\n",
    "                        'timestamp': datetime.now()\n",
    "                    })\n",
    "        \n",
    "        # Delete existing combined data\n",
    "        db[COMBINED_COLLECTION].delete_many({})\n",
    "        \n",
    "        # Insert combined data\n",
    "        if combined_data:\n",
    "            db[COMBINED_COLLECTION].insert_many(combined_data)\n",
    "            print(f\"Combined {len(combined_data)} records in MongoDB\")\n",
    "        \n",
    "        client.close()\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining data in MongoDB: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269d85f",
   "metadata": {},
   "source": [
    "### Load Sample from Combined Data into Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1b1c20a-d08f-4343-8f1a-46f8456fac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_sample_from_mongodb(collection_name, sample_size=1000):\n",
    "    \"\"\"Load a random sample from MongoDB to Spark DataFrame\"\"\"\n",
    "    try:\n",
    "        # Connect to MongoDB\n",
    "        client = pymongo.MongoClient(MONGO_URI)\n",
    "        db = client[DB_NAME]\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        # Count documents\n",
    "        total_docs = collection.count_documents({})\n",
    "        \n",
    "        if total_docs == 0:\n",
    "            print(f\"No documents found in {collection_name}\")\n",
    "            return None\n",
    "        \n",
    "        # Determine sample size\n",
    "        actual_sample_size = min(sample_size, total_docs)\n",
    "        \n",
    "        # Get random sample\n",
    "        random_sample = list(collection.aggregate([\n",
    "            {\"$match\": {\"text\": {\"$exists\": True}, \"label\": {\"$exists\": True}}},\n",
    "            {\"$sample\": {\"size\": actual_sample_size}}\n",
    "        ]))\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        pandas_df = pd.DataFrame(random_sample)\n",
    "        \n",
    "        # Keep only necessary columns\n",
    "        if 'text' in pandas_df.columns and 'label' in pandas_df.columns:\n",
    "            pandas_df = pandas_df[['text', 'label']]\n",
    "            \n",
    "            # Convert to Spark DataFrame\n",
    "            spark_df = spark.createDataFrame(pandas_df)\n",
    "            print(f\"Loaded {len(pandas_df)} random samples from {collection_name}\")\n",
    "            \n",
    "            client.close()\n",
    "            return spark_df\n",
    "        else:\n",
    "            print(\"Required columns not found in the data\")\n",
    "            client.close()\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading random sample from MongoDB: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ead47",
   "metadata": {},
   "source": [
    "### Training & Evaluating Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36dd1684-211b-41a3-8b65-18e277faec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(df, iterations=1):\n",
    "    \"\"\"Train and evaluate the model with multiple attempts if needed\"\"\"\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        try:\n",
    "            print(f\"Training attempt {i+1}/{iterations}\")\n",
    "            \n",
    "            # Clean data\n",
    "            clean_df = df.dropna(subset=[\"text\", \"label\"])\n",
    "            clean_df = clean_df.filter(length(col(\"text\")) > 10)\n",
    "            clean_df = clean_df.withColumn(\"text\", lower(col(\"text\")))\n",
    "            \n",
    "            # Convert string labels to numeric indices\n",
    "            label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").setHandleInvalid(\"skip\")\n",
    "            \n",
    "            # Text processing pipeline\n",
    "            tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "            remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "            \n",
    "            # Use CountVectorizer\n",
    "            vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\", \n",
    "                                       vocabSize=1000, minDF=2.0)\n",
    "            \n",
    "            # Logistic Regression model\n",
    "            lr = LogisticRegression(maxIter=15, regParam=0.05, elasticNetParam=0.7,\n",
    "                                  labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
    "            \n",
    "            # Create pipeline\n",
    "            pipeline = Pipeline(stages=[label_indexer, tokenizer, remover, vectorizer, lr])\n",
    "            \n",
    "            # Split the data - use different random seeds for each attempt\n",
    "            train_data, test_data = clean_df.randomSplit([0.8, 0.2], seed=42+i)\n",
    "            \n",
    "            # Fit the model\n",
    "            model = pipeline.fit(train_data)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.transform(test_data)\n",
    "            \n",
    "            # Evaluate model\n",
    "            evaluator = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"indexedLabel\", \n",
    "                predictionCol=\"prediction\", \n",
    "                metricName=\"accuracy\"\n",
    "            )\n",
    "            \n",
    "            accuracy = evaluator.evaluate(predictions)\n",
    "            print(f\"Model Accuracy (Attempt {i+1}): {accuracy:.4f}\")\n",
    "            \n",
    "            # Keep track of the best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                \n",
    "            # If accuracy is good enough, break early\n",
    "            if accuracy > 0.65:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training attempt {i+1}: {str(e)}\")\n",
    "    \n",
    "    return best_model, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0242a",
   "metadata": {},
   "source": [
    "### Predictions on GNews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f97579cd-04d5-4357-8b3e-06fed37edd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gnews_data_from_gcs_in_memory(model, accuracy, bucket_name=GCP_BUCKET_NAME, file_path=GCS_GNEWS_FILE):\n",
    "    \"\"\"Make predictions on gnews data using in-memory approach\"\"\"\n",
    "    if accuracy <= 0.5:\n",
    "        print(\"Model accuracy below threshold (0.5). Skipping predictions.\")\n",
    "        return False\n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        from google.cloud import storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(file_path)\n",
    "        \n",
    "        print(f\"Downloading content from GCS: {bucket_name}/{file_path}\")\n",
    "        # Download content directly to memory\n",
    "        content = blob.download_as_string()\n",
    "        \n",
    "        # Use StringIO to create a file-like object\n",
    "        import io\n",
    "        csv_file = io.StringIO(content.decode('utf-8'))\n",
    "        \n",
    "        # Read with pandas\n",
    "        print(\"Loading data with pandas\")\n",
    "        pandas_df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Ensure we have the required columns\n",
    "        required_columns = [\"text\"]\n",
    "        if not all(col in pandas_df.columns for col in required_columns):\n",
    "            print(f\"GNews dataset missing required columns. Available columns: {pandas_df.columns}\")\n",
    "            return False\n",
    "        \n",
    "        # Select only needed columns and limit to 50 rows\n",
    "        available_columns = [\"text\"]\n",
    "        if \"source\" in pandas_df.columns:\n",
    "            available_columns.append(\"source\")\n",
    "            \n",
    "        pandas_sample = pandas_df[available_columns].head(50)\n",
    "        \n",
    "        # Add a dummy label column (required by the model)\n",
    "        pandas_sample['label'] = 0\n",
    "        \n",
    "        # Basic text cleaning for prediction\n",
    "        pandas_sample = pandas_sample[pandas_sample['text'].str.len() > 10]\n",
    "        pandas_sample['text'] = pandas_sample['text'].str.lower()\n",
    "        \n",
    "        # Initialize Spark\n",
    "        spark = initialize_spark_with_gcp()\n",
    "        \n",
    "        # Convert to Spark DataFrame\n",
    "        print(\"Converting to Spark DataFrame\")\n",
    "        gnews_sample = spark.createDataFrame(pandas_sample)\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Making predictions\")\n",
    "        predictions = model.transform(gnews_sample)\n",
    "        \n",
    "        # Get the label indexer to map back prediction indices to original labels\n",
    "        from pyspark.ml.feature import StringIndexer\n",
    "        label_indexer_model = None\n",
    "        for stage in model.stages:\n",
    "            if isinstance(stage, StringIndexer):\n",
    "                label_indexer_model = stage\n",
    "                break\n",
    "        \n",
    "        # Select relevant columns including predicted label\n",
    "        from pyspark.sql.functions import col\n",
    "        select_cols = [col(c) for c in available_columns]\n",
    "        select_cols.append(col(\"prediction\").alias(\"predicted_label_idx\"))\n",
    "        \n",
    "        # Add original column names back to result\n",
    "        result_df = predictions.select(*select_cols)\n",
    "        \n",
    "        # If we have the label indexer model, map predictions back to original labels\n",
    "        if label_indexer_model is not None:\n",
    "            # Get the mapping of index to label\n",
    "            label_mapping = {idx: label for idx, label in enumerate(label_indexer_model.labels)}\n",
    "            \n",
    "            # Register user-defined function to map indices to labels\n",
    "            from pyspark.sql.functions import udf\n",
    "            from pyspark.sql.types import StringType\n",
    "            \n",
    "            @udf(StringType())\n",
    "            def idx_to_label(idx):\n",
    "                return label_mapping.get(float(idx), \"unknown\")\n",
    "            \n",
    "            result_df = result_df.withColumn(\"predicted_label\", idx_to_label(col(\"predicted_label_idx\")))\n",
    "            result_df = result_df.drop(\"predicted_label_idx\")\n",
    "        \n",
    "        # Collect results from Spark (force evaluation)\n",
    "        print(\"Collecting results\")\n",
    "        results_pandas = result_df.toPandas()\n",
    "        \n",
    "        # Save results back to GCS\n",
    "        print(\"Saving results back to GCS\")\n",
    "        output_filename = f\"gnews_predictions_{int(accuracy*100)}.csv\"\n",
    "        csv_buffer = io.StringIO()\n",
    "        results_pandas.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "        # Upload the results\n",
    "        output_blob = bucket.blob(f\"Task2/results/{output_filename}\")\n",
    "        output_blob.upload_from_string(csv_buffer.getvalue())\n",
    "        \n",
    "        # Save to MongoDB\n",
    "        print(\"Saving to MongoDB\")\n",
    "        client = pymongo.MongoClient(MONGO_URI)\n",
    "        db = client[DB_NAME]\n",
    "        \n",
    "        # Delete existing predictions\n",
    "        db[PREDICTIONS_COLLECTION].delete_many({})\n",
    "        \n",
    "        # Insert new predictions\n",
    "        db[PREDICTIONS_COLLECTION].insert_many(results_pandas.to_dict('records'))\n",
    "        \n",
    "        print(f\"Saved {len(results_pandas)} predictions to MongoDB and GCS at {bucket_name}/Task2/results/{output_filename}\")\n",
    "        client.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14543db9",
   "metadata": {},
   "source": [
    "### Execution of the Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b61fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 records from GCS:testing694/kaggle_datasets_full.csv to kaggle_data\n",
      "Loaded 1000 records from GCS:testing694/API_datasets_full.csv to api_data\n",
      "Loaded 1000 records from GCS:testing694/gnews_dataset.csv to api_data\n",
      "Combined 959 records in MongoDB\n",
      "Loaded 100 random samples from combined_data\n",
      "Training attempt 1/3\n",
      "Model Accuracy (Attempt 1): 0.5000\n",
      "Training attempt 2/3\n",
      "Model Accuracy (Attempt 2): 0.4000\n",
      "Training attempt 3/3\n",
      "Model Accuracy (Attempt 3): 1.0000\n",
      "Best model accuracy: 1.0000\n",
      "Downloading content from GCS: testing694/gnews_dataset.csv\n",
      "Loading data with pandas\n",
      "Converting to Spark DataFrame\n",
      "Making predictions\n",
      "Collecting results\n",
      "Saving results back to GCS\n",
      "Saving to MongoDB\n",
      "Saved 50 predictions to MongoDB and GCS at testing694/Task2/results/gnews_predictions_100.csv\n"
     ]
    }
   ],
   "source": [
    "def main_with_gcp():\n",
    "    \"\"\"Main workflow using GCP\"\"\"\n",
    "    # Step 1: Load data from GCS into MongoDB\n",
    "    kaggle_loaded = load_csv_from_gcs_to_mongodb(\n",
    "        GCP_BUCKET_NAME, \n",
    "        GCS_KAGGLE_FILE, \n",
    "        KAGGLE_COLLECTION,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    api_loaded = load_csv_from_gcs_to_mongodb(\n",
    "        GCP_BUCKET_NAME, \n",
    "        GCS_API_FILE, \n",
    "        API_COLLECTION\n",
    "    )\n",
    "\n",
    "    api_loaded = load_csv_from_gcs_to_mongodb(\n",
    "        GCP_BUCKET_NAME, \n",
    "        GCS_GNEWS_FILE, \n",
    "        API_COLLECTION\n",
    "    )\n",
    "    \n",
    "    if not (kaggle_loaded and api_loaded):\n",
    "        print(\"Failed to load data from GCS to MongoDB. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Combine and aggregate data (unchanged from your original code)\n",
    "    if not combine_and_aggregate_mongodb_data():\n",
    "        print(\"Failed to combine data in MongoDB. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Load random sample for training (unchanged)\n",
    "    train_df = load_random_sample_from_mongodb(COMBINED_COLLECTION, 100)\n",
    "    \n",
    "    if train_df is None:\n",
    "        print(\"Failed to load training sample. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Train and evaluate model (unchanged)\n",
    "    model, accuracy = train_and_evaluate_model(train_df, iterations=3)\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Failed to train model. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Best model accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: If accuracy > 0.5, make predictions on gnews data from GCS\n",
    "    if accuracy > 0.5:\n",
    "        predict_gnews_data_from_gcs_in_memory(model, accuracy)\n",
    "    else:\n",
    "        print(\"Model accuracy below threshold. Loading more data...\")\n",
    "        # Load more data\n",
    "        larger_train_df = load_random_sample_from_mongodb(COMBINED_COLLECTION, 1000)\n",
    "        \n",
    "        if larger_train_df is not None:\n",
    "            # Try again with more data\n",
    "            model, accuracy = train_and_evaluate_model(larger_train_df, iterations=2)\n",
    "            \n",
    "            if model is not None and accuracy > 0.5:\n",
    "                predict_gnews_data_from_gcs_in_memory(model, accuracy)\n",
    "            else:\n",
    "                print(\"Still couldn't achieve accuracy > 0.5. Stopping.\")\n",
    "        else:\n",
    "            print(\"Failed to load larger training sample. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_with_gcp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DistributedComputing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
